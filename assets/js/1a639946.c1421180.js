"use strict";(self.webpackChunkendlessq=self.webpackChunkendlessq||[]).push([[800],{2899:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>l});var t=s(4848),i=s(8453);const r={},o="ViTEncoder and ViTLayer",a={id:"vision-transformer/sub-modules/sub-modules-encoder",title:"ViTEncoder and ViTLayer",description:"Lets examine how the encoder processes input hidden states layer-by-layer, each adding to the model's understanding of the image. This sequential processing, enriched by the self-attention mechanism, allows the model to integrate information across the entire image, distinguishing Vision Transformers from architectures that focus on local image features.",source:"@site/docs/vision-transformer/sub-modules/sub-modules-encoder.md",sourceDirName:"vision-transformer/sub-modules",slug:"/vision-transformer/sub-modules/sub-modules-encoder",permalink:"/endlessq/docs/vision-transformer/sub-modules/sub-modules-encoder",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"ViTEmbeddings",permalink:"/endlessq/docs/vision-transformer/sub-modules/sub-modules-embeddings"},next:{title:"ViTIntermediate and ViTOutput",permalink:"/endlessq/docs/vision-transformer/sub-modules/sub-modules-output"}},d={},l=[{value:"Constructor and Parameters:",id:"constructor-and-parameters",level:2},{value:"Layer Initialization:",id:"layer-initialization",level:2},{value:"Gradient Checkpointing:",id:"gradient-checkpointing",level:2},{value:"Forward Method of ViTEncoder",id:"forward-method-of-vitencoder",level:2},{value:"Hidden States Processing:",id:"hidden-states-processing",level:2},{value:"Output Generation:",id:"output-generation",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"vitencoder-and-vitlayer",children:"ViTEncoder and ViTLayer"}),"\n",(0,t.jsx)(n.p,{children:"Lets examine how the encoder processes input hidden states layer-by-layer, each adding to the model's understanding of the image. This sequential processing, enriched by the self-attention mechanism, allows the model to integrate information across the entire image, distinguishing Vision Transformers from architectures that focus on local image features."}),"\n",(0,t.jsx)(n.h2,{id:"constructor-and-parameters",children:"Constructor and Parameters:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def __init__(self, config: ViTConfig):\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The constructor for ",(0,t.jsx)(n.code,{children:"ViTEncoder"})," takes a ",(0,t.jsx)(n.code,{children:"ViTConfig"})," object as a parameter. This object holds configuration settings for the encoder, such as the number of layers, attention heads, etc."]}),"\n",(0,t.jsx)(n.h2,{id:"layer-initialization",children:"Layer Initialization:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"self.layer = nn.ModuleList([ViTLayer(config) for _ in range(config.num_hidden_layers)])\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This initializes the layers of the encoder. ",(0,t.jsx)(n.code,{children:"ViTLayer"})," is another class that defines a single layer of the transformer. The number of layers (",(0,t.jsx)(n.code,{children:"num_hidden_layers"}),") is specified in the configuration. These layers are stored in a ",(0,t.jsx)(n.code,{children:"ModuleList"}),", allowing PyTorch to recognize them as submodules of the encoder."]}),"\n",(0,t.jsx)(n.h2,{id:"gradient-checkpointing",children:"Gradient Checkpointing:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"self.gradient_checkpointing = False\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Gradient checkpointing is an optimization technique to reduce memory usage during training. By default, it's set to ",(0,t.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"forward-method-of-vitencoder",children:"Forward Method of ViTEncoder"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def forward(\n    self,\n    hidden_states: torch.Tensor,\n    head_mask: Optional[torch.Tensor] = None,\n    output_attentions: bool = False,\n    output_hidden_states: bool = False,\n    return_dict: bool = True,\n):\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"forward"})," method is where the actual processing happens. It takes several arguments, including the ",(0,t.jsx)(n.code,{children:"hidden_states"})," from the previous layer (or embeddings), an optional ",(0,t.jsx)(n.code,{children:"head_mask"})," for attention layers, and flags to control the outputs."]}),"\n",(0,t.jsx)(n.h2,{id:"hidden-states-processing",children:"Hidden States Processing:"}),"\n",(0,t.jsx)(n.p,{children:"The hidden states are passed through each layer of the encoder sequentially. In each layer, the self-attention mechanism and feed-forward neural network are applied."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"for i, layer_module in enumerate(self.layer):\n    layer_outputs = layer_module(\n        hidden_states, \n        head_mask[i] if head_mask is not None else None, \n        output_attentions\n    )\n    hidden_states = layer_outputs[0]\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This loop iterates over each layer, processing the hidden states. The ",(0,t.jsx)(n.code,{children:"head_mask"})," and ",(0,t.jsx)(n.code,{children:"output_attentions"})," are also passed to each layer."]}),"\n",(0,t.jsx)(n.h2,{id:"output-generation",children:"Output Generation:"}),"\n",(0,t.jsxs)(n.p,{children:["The encoder can output the last layer's hidden states, all layers' hidden states, and attention weights, based on the flags ",(0,t.jsx)(n.code,{children:"output_hidden_states"})," and ",(0,t.jsx)(n.code,{children:"output_attentions"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"if not return_dict:\n    return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Returning a tuple or a ",(0,t.jsx)(n.code,{children:"BaseModelOutput"})," object containing the requested outputs."]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);