"use strict";(self.webpackChunkendlessq=self.webpackChunkendlessq||[]).push([[9158],{7850:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>d});var o=i(4848),s=i(8453);const t={},a="ViT for Masked Image Modeling",r={id:"vision-transformer/masked-modeling",title:"ViT for Masked Image Modeling",description:"This class is designed for the task of masked image modeling, it reconstructs masked portions of images, which is a form of self-supervised learning where the model learns to predict the parts of the image that have been masked.",source:"@site/docs/vision-transformer/masked-modeling.md",sourceDirName:"vision-transformer",slug:"/vision-transformer/masked-modeling",permalink:"/endlessq/docs/vision-transformer/masked-modeling",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"The Main Class",permalink:"/endlessq/docs/vision-transformer/vit-model"},next:{title:"Pooling and Classification Heads",permalink:"/endlessq/docs/vision-transformer/pooling-classification-head"}},l={},d=[{value:"Initializing the ViT Model",id:"initializing-the-vit-model",level:2},{value:"Decoder Initialization",id:"decoder-initialization",level:2},{value:"Weights Initialization and Final Processing",id:"weights-initialization-and-final-processing",level:2},{value:"Forward Method",id:"forward-method",level:2},{value:"Validating Configuration",id:"validating-configuration",level:2},{value:"Processing through ViTModel",id:"processing-through-vitmodel",level:2},{value:"Extracting the Sequence Output",id:"extracting-the-sequence-output",level:2},{value:"Reshaping for Reconstruction",id:"reshaping-for-reconstruction",level:2},{value:"Reconstructing Pixel Values",id:"reconstructing-pixel-values",level:2},{value:"Computing Reconstruction Loss",id:"computing-reconstruction-loss",level:2},{value:"Returning the Output",id:"returning-the-output",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"vit-for-masked-image-modeling",children:"ViT for Masked Image Modeling"}),"\n",(0,o.jsx)(n.p,{children:"This class is designed for the task of masked image modeling, it reconstructs masked portions of images, which is a form of self-supervised learning where the model learns to predict the parts of the image that have been masked."}),"\n",(0,o.jsxs)(n.h1,{id:"vitformaskedimagemodeling-class-exploration",children:[(0,o.jsx)(n.code,{children:"ViTForMaskedImageModeling"})," Class Exploration"]}),"\n",(0,o.jsx)(n.h2,{id:"initializing-the-vit-model",children:"Initializing the ViT Model"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"self.vit = ViTModel(config, add_pooling_layer=False, use_mask_token=True)\n"})}),"\n",(0,o.jsxs)(n.p,{children:["This line creates an instance of ",(0,o.jsx)(n.code,{children:"ViTModel"}),". It specifies not to add an additional pooling layer (",(0,o.jsx)(n.code,{children:"add_pooling_layer=False"}),") and to use a mask token (",(0,o.jsx)(n.code,{children:"use_mask_token=True"}),"). The mask token is crucial for masked image modeling."]}),"\n",(0,o.jsx)(n.h2,{id:"decoder-initialization",children:"Decoder Initialization"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"self.decoder = nn.Sequential(\n    nn.Conv2d(\n        in_channels=config.hidden_size,\n        out_channels=config.encoder_stride2 * config.num_channels,\n        kernel_size=1,\n    ),\n    nn.PixelShuffle(config.encoder_stride),\n)\n"})}),"\n",(0,o.jsx)(n.p,{children:"Initializes a sequential decoder consisting of a 2D convolutional layer and a pixel shuffle operation. This decoder is responsible for reconstructing the original image from the encoded representations."}),"\n",(0,o.jsx)(n.h2,{id:"weights-initialization-and-final-processing",children:"Weights Initialization and Final Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"self.post_init()\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Calls the ",(0,o.jsx)(n.code,{children:"post_init"})," method to initialize the weights and apply any final configurations to the model."]}),"\n",(0,o.jsx)(n.h2,{id:"forward-method",children:"Forward Method"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def forward(\n    self,\n    pixel_values: Optional[torch.Tensor] = None,\n    bool_masked_pos: Optional[torch.BoolTensor] = None,\n    head_mask: Optional[torch.Tensor] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    interpolate_pos_encoding: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n) -> Union[tuple, MaskedImageModelingOutput]:\n"})}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"forward"})," method defines the processing of input images (pixel_values) through the ViT model and the decoder for masked image modeling."]}),"\n",(0,o.jsx)(n.h2,{id:"validating-configuration",children:"Validating Configuration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"if bool_masked_pos is not None and (self.config.patch_size != self.config.encoder_stride):\n    raise ValueError(...)\n"})}),"\n",(0,o.jsx)(n.p,{children:"Ensures that if a boolean mask for the positions (bool_masked_pos) is provided, the patch size and the encoder stride are equal. This is required for the reconstruction phase to align correctly with the input image."}),"\n",(0,o.jsx)(n.h2,{id:"processing-through-vitmodel",children:"Processing through ViTModel"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"outputs = self.vit(\n    pixel_values,\n    bool_masked_pos=bool_masked_pos,\n    head_mask=head_mask,\n    ...\n)\n"})}),"\n",(0,o.jsxs)(n.p,{children:["The input pixel values, along with optional parameters like the boolean masked positions, head mask, etc., are processed through the ",(0,o.jsx)(n.code,{children:"ViTModel"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"extracting-the-sequence-output",children:"Extracting the Sequence Output"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"sequence_output = outputs[0]\n"})}),"\n",(0,o.jsx)(n.p,{children:"Retrieves the sequence output from the ViT model's output, which contains the encoded representations of the input image."}),"\n",(0,o.jsx)(n.h2,{id:"reshaping-for-reconstruction",children:"Reshaping for Reconstruction"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"sequence_output = sequence_output[:, 1:]\nbatch_size, sequence_length, num_channels = sequence_output.shape\nheight = width = math.floor(sequence_length0.5)\nsequence_output = sequence_output.permute(0, 2, 1).reshape(batch_size, num_channels, height, width)\n"})}),"\n",(0,o.jsx)(n.p,{children:"The sequence output is reshaped to prepare for the reconstruction of the original image. The first token (usually the [CLS] token) is removed as it's not needed for reconstruction."}),"\n",(0,o.jsx)(n.h2,{id:"reconstructing-pixel-values",children:"Reconstructing Pixel Values"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"reconstructed_pixel_values = self.decoder(sequence_output)\n"})}),"\n",(0,o.jsx)(n.p,{children:"The decoder is used to reconstruct the pixel values from the encoded sequence output."}),"\n",(0,o.jsx)(n.h2,{id:"computing-reconstruction-loss",children:"Computing Reconstruction Loss"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"if bool_masked_pos is not None:\n    ...\n"})}),"\n",(0,o.jsx)(n.p,{children:"If a mask is provided, this section computes the reconstruction loss, which quantifies how well the model reconstructs the masked parts of the image."}),"\n",(0,o.jsx)(n.h2,{id:"returning-the-output",children:"Returning the Output"}),"\n",(0,o.jsx)(n.p,{children:"The method returns the reconstruction loss and the reconstructed pixel values, along with other optional outputs like hidden states and attentions, based on specified flags."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}}}]);