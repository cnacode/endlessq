"use strict";(self.webpackChunkendlessq=self.webpackChunkendlessq||[]).push([[5876],{7005:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>c,frontMatter:()=>a,metadata:()=>r,toc:()=>l});var s=i(4848),t=i(8453);const a={},o="ViTEmbeddings",r={id:"vision-transformer/sub-modules/sub-modules-embeddings",title:"ViTEmbeddings",description:"The class handles the embedding of image patches and adds necessary tokens and positional information, setting the stage for the operations that follow in the ViT architecture.",source:"@site/docs/vision-transformer/sub-modules/sub-modules-embeddings.md",sourceDirName:"vision-transformer/sub-modules",slug:"/vision-transformer/sub-modules/sub-modules-embeddings",permalink:"/endlessq/docs/vision-transformer/sub-modules/sub-modules-embeddings",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"ViTSelfAttention and ViTAttention",permalink:"/endlessq/docs/vision-transformer/sub-modules/sub-modules-attention"},next:{title:"ViTEncoder and ViTLayer",permalink:"/endlessq/docs/vision-transformer/sub-modules/sub-modules-encoder"}},d={},l=[{value:"Constructor and Parameters:",id:"constructor-and-parameters",level:2},{value:"CLS Token Embedding",id:"cls-token-embedding",level:2},{value:"Optional Mask Token:",id:"optional-mask-token",level:2},{value:"Patch Embeddings:",id:"patch-embeddings",level:2},{value:"Positional Embeddings:",id:"positional-embeddings",level:2},{value:"Dropout Layer:",id:"dropout-layer",level:2},{value:"Configuration Storage:",id:"configuration-storage",level:2},{value:"Interpolating Positional Encodings",id:"interpolating-positional-encodings",level:2},{value:"Forward Pass",id:"forward-pass",level:2},{value:"Extracting Embeddings:",id:"extracting-embeddings",level:3},{value:"Applying Mask (If Present):",id:"applying-mask-if-present",level:3},{value:"Adding the [CLS] Token:",id:"adding-the-cls-token",level:3},{value:"Positional Encoding Addition:",id:"positional-encoding-addition",level:3},{value:"Applying Dropout:",id:"applying-dropout",level:3}];function h(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"vitembeddings",children:"ViTEmbeddings"}),"\n",(0,s.jsx)(n.p,{children:"The class handles the embedding of image patches and adds necessary tokens and positional information, setting the stage for the operations that follow in the ViT architecture.\nIt prepares the input image for subsequent processing stages, ensuring that crucial information about image patches and their positions is effectively encapsulated in the embeddings."}),"\n",(0,s.jsx)(n.h2,{id:"constructor-and-parameters",children:"Constructor and Parameters:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def __init__(self, config: ViTConfig, use_mask_token: bool = False):\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The constructor of ",(0,s.jsx)(n.code,{children:"ViTEmbeddings"})," takes two parameters: a configuration object (",(0,s.jsx)(n.code,{children:"config"}),"), which contains all the necessary settings for the model, and a boolean ",(0,s.jsx)(n.code,{children:"use_mask_token"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"use_mask_token"})," flag determines whether an additional mask token should be used, which is particularly relevant for tasks like masked image modeling."]}),"\n",(0,s.jsx)(n.h2,{id:"cls-token-embedding",children:"CLS Token Embedding"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n"})}),"\n",(0,s.jsx)(n.p,{children:"Here, a class (CLS) token is created and registered as a parameter of the model. It's initialized randomly and will be learned during training. This token is crucial for tasks like image classification, where the representation of this token is used as the overall representation of the image."}),"\n",(0,s.jsxs)(n.p,{children:["It's initialized randomly and is fine-tuned during the training process. In tasks like image classification, the final state of the ",(0,s.jsx)(n.code,{children:"cls_token"})," serves as a representation of the entire image. The model uses this aggregated information to classify the image."]}),"\n",(0,s.jsx)(n.p,{children:"It interacts with all patch embeddings through the self-attention mechanism, aggregating global information from the entire image."}),"\n",(0,s.jsx)(n.h2,{id:"optional-mask-token",children:"Optional Mask Token:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"self.mask_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size)) if use_mask_token else None\n"})}),"\n",(0,s.jsxs)(n.p,{children:["If ",(0,s.jsx)(n.code,{children:"use_mask_token"})," is ",(0,s.jsx)(n.code,{children:"True"}),", a mask token is also initialized. This is an approach inspired by the masked language modeling concept in NLP models like BERT. In masked image modeling:"]}),"\n",(0,s.jsx)(n.p,{children:"If this flag is set, certain parts of the input image (specific patches) are intentionally hidden (masked) during the training process, the model initializes a special token \u2013 the mask token. This token is used to replace the embeddings of the masked image patches."}),"\n",(0,s.jsx)(n.p,{children:"Then, during training, the model learns to predict these masked parts, encouraging the development of a deeper, context-driven understanding of the unmasked parts of the image."}),"\n",(0,s.jsx)(n.h2,{id:"patch-embeddings",children:"Patch Embeddings:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"self.patch_embeddings = ViTPatchEmbeddings(config)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["It essentially projects the flattened patches into a higher-dimensional space (the embedding space). The ",(0,s.jsx)(n.code,{children:"ViTPatchEmbeddings"})," class is crucial for converting image pixels into a form suitable for processing by the transformer model."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Processing Image Patches"}),":\nIt takes raw pixel values of the image and divides the image into fixed-size patches.\nEach patch is then flattened and projected into an embedding space, essentially converting a 2D image patch into a 1D vector."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Embedding Projection"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"self.projection = nn.Conv2d(...)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["This is implemented using a 2D convolution layer (",(0,s.jsx)(n.code,{children:"nn.Conv2d"}),") which acts as a linear projection. This approach is computationally efficient and maintains spatial locality."]}),"\n",(0,s.jsx)(n.h2,{id:"positional-embeddings",children:"Positional Embeddings:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, config.hidden_size))\n"})}),"\n",(0,s.jsx)(n.p,{children:"Positional embeddings are crucial in Transformers as they add information about the position of each patch in the image. These embeddings are learnable parameters and are added to the patch embeddings to provide the model with spatial context."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"nn.Parameter"})," is a core concept in PyTorch, used to define variables that should be considered as model parameters. Variables wrapped in ",(0,s.jsx)(n.code,{children:"nn.Parameter"})," are trainable and are adjusted during the back-propagation process. These parameters are part of the computation graph, and PyTorch keeps track of their gradients automatically."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"torch.randn"})," generates a tensor with elements sampled from a normal distribution (mean = 0, standard deviation = 1)."]}),"\n",(0,s.jsxs)(n.p,{children:["The first dimension ",(0,s.jsx)(n.code,{children:"1"})," is for the batch size, assuming a single image (or a batch of images processed as one).\n",(0,s.jsx)(n.code,{children:"num_patches + 1"})," represents the total number of patches plus one for the [CLS] token.\n",(0,s.jsx)(n.code,{children:"config.hidden_size"})," defines the size of the embedding vector for each patch."]}),"\n",(0,s.jsx)(n.h2,{id:"dropout-layer",children:"Dropout Layer:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"self.dropout = nn.Dropout(config.hidden_dropout_prob)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Dropout is used as a regularization technique to prevent overfitting. The dropout probability is specified in the configuration."}),"\n",(0,s.jsx)(n.p,{children:"During training, it randomly sets a fraction of the input units to zero at each update step, which helps in breaking up happenstance correlations in the training data."}),"\n",(0,s.jsx)(n.p,{children:"By preventing complex co-adaptations on the training data, dropout forces the model to learn more robust features that are useful in conjunction with many different random subsets of the other neurons."}),"\n",(0,s.jsx)(n.h2,{id:"configuration-storage",children:"Configuration Storage:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"self.config = config\n"})}),"\n",(0,s.jsx)(n.p,{children:"Storing the configuration object within the class allows easy access to model settings throughout the embedding process."}),"\n",(0,s.jsx)(n.h2,{id:"interpolating-positional-encodings",children:"Interpolating Positional Encodings"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n"})}),"\n",(0,s.jsx)(n.p,{children:"This method is a crucial component when dealing with images of various resolutions.\nThe original positional encodings are trained on a specific image size.\nWhen input images are of different sizes, this method allows for the resizing of these positional encodings to match the new dimensions.\nIt ensures that the model can handle images of various resolutions, which is particularly important in real-world applications where input image sizes can vary."}),"\n",(0,s.jsx)(n.p,{children:"When input images are of different sizes than the ones used in training, this function interpolates the positional embeddings to align with the new dimensions. This ensures that the model remains effective and accurate even when applied to images of sizes it wasn't explicitly trained on, enhancing its utility in real-world applications where input image sizes can vary significantly."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"The Process of Interpolation"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Adjusting to New Image Sizes:\nThe first step involves computing the new dimensions (h0 and w0) for height and width, respectively. These are derived based on the input image's size relative to the patch size configured in the model. This step is pivotal as it sets the stage for the rest of the interpolation process."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Reshaping and Interpolating Positional Embeddings:\nUsing bicubic interpolation, a method known for its ability to resize images while maintaining their quality, the positional embeddings are reshaped. This reshaping ensures that the positional information aligns with the new dimensions of the image, preserving the spatial context essential for the model's understanding."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Maintaining the Relevance of the [CLS] Token:\nThe [CLS] token (for classification tasks) is concatenated with these adjusted positional embeddings. This ensures that despite the change in image size, the [CLS] token continues to effectively represent the whole image. The model thereby maintains its capability to use this token for tasks like image classification, regardless of image resolution."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"forward-pass",children:"Forward Pass"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def forward(self, pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.BoolTensor] = None, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n"})}),"\n",(0,s.jsx)(n.p,{children:"The forward method is where the actual embedding process happens for a given input."}),"\n",(0,s.jsx)(n.p,{children:"the process begins with extracting the shape of the input pixel values, identifying the batch size, number of channels, height, and width."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"batch_size, num_channels, height, width = pixel_values.shape\n"})}),"\n",(0,s.jsx)(n.h3,{id:"extracting-embeddings",children:"Extracting Embeddings:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"embeddings = self.patch_embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The forward method calls ",(0,s.jsx)(n.code,{children:"self.patch_embeddings"})," to convert the input pixel values into patch embeddings. These embeddings are essentially flattened, linear projections of the image patches."]}),"\n",(0,s.jsx)(n.p,{children:"If interpolate_pos_encoding is set to True, this step also adjusts the positional encodings for different image resolutions."}),"\n",(0,s.jsx)(n.h3,{id:"applying-mask-if-present",children:"Applying Mask (If Present):"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"if bool_masked_pos is not None:\n    seq_length = embeddings.shape[1]\n    mask_tokens = self.mask_token.expand(batch_size, seq_length, -1)\n    # replace the masked visual tokens by mask_tokens\n    mask = bool_masked_pos.unsqueeze(-1).type_as(mask_tokens)\n    embeddings = embeddings * (1.0 - mask) + mask_tokens * mask\n"})}),"\n",(0,s.jsxs)(n.p,{children:["If ",(0,s.jsx)(n.code,{children:"bool_masked_pos"})," is provided, which indicates masked positions for masked image modeling, the method applies these masks. It does so by replacing the embeddings at the masked positions with the mask token's embeddings."]}),"\n",(0,s.jsx)(n.p,{children:"This process is critical for tasks like self-supervised learning where parts of the input are intentionally hidden during training."}),"\n",(0,s.jsx)(n.h3,{id:"adding-the-cls-token",children:"Adding the [CLS] Token:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"cls_tokens = self.cls_token.expand(batch_size, -1, -1)\nembeddings = torch.cat((cls_tokens, embeddings), dim=1)\n"})}),"\n",(0,s.jsx)(n.p,{children:"The [CLS] token's embedding is then added to the sequence of patch embeddings. It's included at the beginning. This token plays an important role in tasks like classification, where its final state represents the aggregate understanding of the entire image."}),"\n",(0,s.jsx)(n.h3,{id:"positional-encoding-addition",children:"Positional Encoding Addition:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"if interpolate_pos_encoding:\n    embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\nelse:\n    embeddings = embeddings + self.position_embeddings\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Next, either the original or interpolated positional embeddings (based on the ",(0,s.jsx)(n.code,{children:"interpolate_pos_encoding"})," flag) are added to these embeddings, infusing them with positional context."]}),"\n",(0,s.jsx)(n.p,{children:"Depending on whether interpolation is active (for varying image resolutions), this adds either the original or interpolated positional encodings to provide spatial context to the embeddings."}),"\n",(0,s.jsx)(n.h3,{id:"applying-dropout",children:"Applying Dropout:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"embeddings = self.dropout(embeddings)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Finally, a dropout layer is applied for regularization, helping prevent over-fitting."}),"\n",(0,s.jsx)(n.p,{children:"This layer helps in regularizing the model, reducing the risk of over-fitting by randomly deactivating a subset of neurons during training."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"return embeddings\n"})}),"\n",(0,s.jsx)(n.p,{children:"These embeddings, now enriched with CLS tokens, masked token adjustments (if applicable), and positional information, are ready for the subsequent layers of the Vision Transformer model."}),"\n",(0,s.jsx)(n.h1,{id:"vitpatchembeddings",children:"VITPatchEmbeddings"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"ViTPatchEmbeddings"})," class is responsible for converting the raw pixel values of an image into a sequence of patch embeddings. It's a crucial component of the Vision Transformer model, as it prepares the input image for subsequent processing by the transformer layers."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"image_size, patch_size = config.image_size, config.patch_size\nnum_channels, hidden_size = config.num_channels, config.hidden_size\n"})}),"\n",(0,s.jsx)(n.p,{children:"First, the dimensions of the image and patches, the number of channels in the input images, and the size of the hidden layer are extracted from the configuration."}),"\n",(0,s.jsx)(n.p,{children:"Then, it processes the image and patch sizes to ensure they are iterables (tuples)."}),"\n",(0,s.jsx)(n.p,{children:"image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\npatch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)"}),"\n",(0,s.jsx)(n.p,{children:"This step ensures flexibility in the input dimensions, accommodating both uniform and non-uniform image and patch sizes."}),"\n",(0,s.jsx)(n.p,{children:"After that, number of patches in each image is then calculated. This is done by dividing the height and width of the image by the height and width of each patch, respectively. This calculation determines how many patches an image will be divided into, based on the image and patch sizes."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"num_patches = (image_size[0] // patch_size[0]) * (image_size[1] // patch_size[1])\n"})}),"\n",(0,s.jsx)(n.p,{children:"The class then initializes its attributes with these values."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"self.image_size = image_size\nself.patch_size = patch_size\nself.num_channels = num_channels\nself.num_patches = num_patches\n"})}),"\n",(0,s.jsx)(n.p,{children:"These attributes hold the essential information about the image dimensions, patch sizes, and number of patches, which are used throughout the patch embedding process."}),"\n",(0,s.jsx)(n.p,{children:"In the next step, A 2D convolutional layer is set up as the projection mechanism. This layer is responsible for projecting the raw pixel values of the image into a higher-dimensional space, essentially converting the 2D image patches into 1D vectors."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n"})}),"\n",(0,s.jsx)(n.p,{children:"The convolution layer accomplishes this by applying a linear projection to the image patches. In any convolution layer the kernel essentially slides over the image and multiplies the pixel values in the kernel's receptive field by the weights in the kernel."}),"\n",(0,s.jsx)(n.p,{children:"Here the hidden_size is the number of output channels, if the image only has 1 size then the output will be a 2D tensor, if the image has more than 1 channel then the output will be a 3D tensor."}),"\n",(0,s.jsx)(n.p,{children:"In other words it takes the pixels and then its kernel will slide over the image and apply a linear transformation to the pixels in the kernel's receptive field."}),"\n",(0,s.jsx)(n.p,{children:"It's a computationally efficient approach that maintains spatial locality, ensuring that the model can effectively capture spatial relationships between patches."})]})}function c(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}}}]);